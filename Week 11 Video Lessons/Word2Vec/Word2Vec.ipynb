{"cells":[{"cell_type":"markdown","metadata":{"id":"KrIVwBdx_NRG"},"source":["### Week 11: Day 3 â€“ Word2Vec  Word2vec Detailed Explanation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jbqcx3Eq_NRR"},"outputs":[],"source":["#Use gensim to load a word2vec model pretrained on google news and perform some simple actions with the word vectors.\n","\n","import gensim.downloader as api\n","wv = api.load('word2vec-google-news-300')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czcdBRgG_NRX","outputId":"390ab766-6f9e-4644-ee1b-28d17a2eb598"},"outputs":[{"name":"stdout","output_type":"stream","text":["</s>\n","in\n","for\n","that\n","is\n","on\n","##\n","The\n","with\n","said\n"]}],"source":["# 10 words in the vocabulary in the model\n","\n","for i , word in enumerate(wv.vocab):\n","    if i == 10:\n","         break\n","    print(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3M70cMIi_NRb","outputId":"8f62607b-a152-440c-a7e1-73f23b57bd0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n"," -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n","  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n"," -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n","  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n","  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n","  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n","  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n","  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n","  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n","  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n","  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n"," -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n"," -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n"," -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n","  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n","  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n"," -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n"," -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n"," -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n","  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n","  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n","  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n"," -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n"," -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n","  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n"," -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n","  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n"," -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n"," -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n","  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n"," -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n"," -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n","  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n"," -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n","  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n","  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n"," -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n"," -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n"," -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n"," -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n"," -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n","  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n","  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n","  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n"," -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n","  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n","  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n"," -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n"," -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n"," -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n","  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n","  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n"," -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n","  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n"," -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n","  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n","  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n"," -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n","  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n","  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n","  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n","  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n","  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n"," -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n","  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n"," -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n","  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n","  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n"," -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n","  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n","  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n","  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n"," -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n"," -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n"]}],"source":["# vECTOR OF A PARTICULAR WORD\n","vec_king = wv['king']\n","print(vec_king)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2gwxiDm_NRd","outputId":"5bbd17dc-269f-4330-8cfa-effcfce4145e"},"outputs":[{"name":"stdout","output_type":"stream","text":["'car'\t'minivan'\t0.690704\n","'car'\t'bicycle'\t0.536448\n","'car'\t'airplane'\t0.424356\n","'car'\t'cereal'\t0.139247\n","'car'\t'communism'\t0.058203\n"]}],"source":["# We want to find out the similarity between these words\n","pairs=[\n","    ('car', 'minivan'),    # a minivan is a kind of car\n","    ('car', 'bicycle'),    # still a wheeled vehicle\n","    ('car', 'airplane'),   # ok, no wheels, but still a vehicle\n","    ('car', 'cereal'),     # ... and so on\n","    ('car', 'communism'),\n","]\n","     \n","for w1,w2 in pairs:\n","     print('%r\\t%r\\t%2f'% (w1,w2, wv.similarity(w1,w2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJqUCIYM_NRf","outputId":"94af5a9a-5056-4d56-b8a9-bc9ce1a739f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('SUV', 0.8532191514968872), ('vehicle', 0.8175784349441528), ('pickup_truck', 0.7763689160346985), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565719485282898)]\n"]}],"source":["# We want to check the top 5 similar words to 'car' and 'minivan' \n","print(wv.most_similar(positive=['car', 'minivan'], topn=5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53txgkAF_NRf"},"outputs":[],"source":["# It returns the odd words\n","print(wv.doesnt_match(['fire', 'water','land','sea', 'air','car']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgvasdxW_NRh"},"outputs":[],"source":["# Import neccessary libraries\n","import json\n","import pandas as pd\n","import string\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnnJeHVc_NRi"},"outputs":[],"source":["# import dataset\n","data = []\n","\n","for line in open('C:\\\\Users\\\\Feranmi\\\\Desktop\\\\anaconda\\\\DATA_SCIENCE_SOI_VIDEOS\\\\wk11\\\\Cell_Phones_and_Accessories_5.json'):\n","    data.append(json.loads(line))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FuuJbrd_NRj"},"outputs":[],"source":["# covert into a pandas DataFrame\n","print(data[0])\n","df = pd.DataFrame(data)\n","print(len(data))"]},{"cell_type":"markdown","metadata":{"id":"Qz1RR3H-_NRk"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoLwliCH_NRk"},"outputs":[],"source":["# Drop columns and renaming important columns \n","df.head(10)\n","df=df.drop(columns=['reviewerName', 'vote', 'image', 'style'])\n","df=df.rename(columns={'overall': 'rating', 'asin':'productID'},inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Faxgb4rk_NRm"},"outputs":[],"source":["df1 = df.dropna(axis = 0, how ='any', inplace=True)\n","df1.drop duplicates(subset=['rating', 'reviewText'],keep='first',inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYJmL8Ta_NRm"},"outputs":[],"source":["# clean text by removing some punctuations\n","def clean_text(text):\n","    delete_dict = {sp_character: ' ' for sp_character in string.punctuation}\n","    delete_dict[' ']=' '\n","    table = str.maketrans(delete_dict)\n","    text1 = text.translate(table)\n","    #print('cleaned:'+text1)\n","    textArr= text1.split()\n","    text2= ' '.join([w for w in textArr if ( not w.isdigit()and( not))])\n","    \n","    return text2.lower().split(' ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRBtHOuy_NRo"},"outputs":[],"source":["# create 200000 sub sample\n","df2=df1.sample(n=200000)\n","df2['reviewText']=df2['reviewText'].apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPWJGb6m_NRo"},"outputs":[],"source":["sentences = df2['reviewText'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rPM-Tz1_NRp"},"outputs":[],"source":["print(len(sentences))\n","print(sentences[1])\n","print(sentences[2000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvSZtkTr_NRp"},"outputs":[],"source":["# Import library\n","import gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPQJ0Y-M_NRp"},"outputs":[],"source":["\n","from gensim.models.callbacks import CallbackAny2Vec\n","from gensim.models import word2vec\n","\n","#init callback class\n","class callback(CallbackAny2Vec):\n","    '''\n","    Callback to print loss after each epoch\n","    '''\n","    def __init__(self):\n","        self.epoch =0\n","        \n","        def on_epoch_end(self, mode1):\n","            loss = model.get_latest_training_loss()\n","            \n","            if self.epoch == 0:\n","                print('Loss after epoch {}:{}'.format(self.epoch, loss))\n","            elif self.epoch % 100 ==0:\n","                print('Loss after epoch {}: {}'.format(self.epoch,loss- self.loss_previous_step))\n","                \n","                \n","                self.epoch += 1\n","                self.loss_previous_step =loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMj1BS7a_NRq"},"outputs":[],"source":["#creating our word to vec model\n","# init word2vec class\n","w2v_model = Word2Vec(size =300,\n","                    window=15,\n","                    min_count=2,\n","                    workers = 20,\n","                    sg = 1,\n","                    negative =5,\n","                    sample=1e-5)\n","#build vovab\n","w2v_model.build_vocab(sentences)\n","\n","#train the w2v model\n","start= time.time()\n","w2w_model.train(sentences,\n","               total_examples=w2v_model.corpus_count,\n","               epochs=1001,\n","               report_delays=1,\n","               compute_loss = True,#set compute_loss = True\n","               callbacks=[callbacks()])#add the callback class\n","end = time.time()\n","\n","print('elapsedtime in seconds:'+str(end-start))\n","#save the word2vec model\n","w2v_model.save('C:\\\\Users\\\\Feranmi\\\\Desktop\\\\anaconda\\\\DATA_SCIENCE_SOI_VIDEOS\\\\wk11\\\\word2vec.model')\n","#'C:\\\\Users\\\\Feranmi\\\\Desktop\\\\anaconda\\\\DATA_SCIENCE_SOI_VIDEOS\\\\wk11\\\\Cell_Phones_and_Accessories_5.json'"]},{"cell_type":"markdown","metadata":{"id":"w0tsoMup_NRs"},"source":["Let us reload our word2vec model and perform operations using it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkMhfkzW_NRs"},"outputs":[],"source":["\n","reloaded_w2v_mode1 = Word2Vec.load('C:\\\\AmazonReviewsCellPhones\\\\word2vec.mo')\n","words = list(reloaded_w2v_model.wv.vocab)# put vocab into a list\n","print('Vocab size: '+str(len(words)))\n","# find top 3 similar words to the word 'cancellation'\n","w1 = 'cancellation'\n","print(\"Top 3 words similar to cancellation:\",\\\n","      reloaded_w2v_mode1.wv.most_similar(positive = w1,topn =3))\n","# find top 3 similar words to the word 'poor'\n","w1 = 'poor'\n","print(\"Top 3 words similar to poor:\",\\\n","      reloaded_w2v_model.wv.most_similar(positive = w1, topn =3))\n","# Similarity between earphones and headphones\n","print('Similarity between earphones and headphones:'+\\\n","     str(reloaded_w2v_model.wv.similarity(w1='earphones', w2='headphones')))\n","# Similarity between charger and charge\n","print('similarity between charger and charge:'+\\\n","         str(reloaded_w2v_model.wv.similarity(w1='charger', w2='charge  ')))\n"]},{"cell_type":"markdown","metadata":{"id":"nGygyad1_NRt"},"source":["Let us use TSNE(for reducing dimentions in data) to do reduce features Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCuDfhU8_NRu"},"outputs":[],"source":["from sklearn.manifold import TSNE   #final reduction \n","import numpy as np                  #array handling\n","\n","def reduce_dimensions(mode1):\n","    num_dimentions = 2 #final num dimention (2D, 3D, etc)\n","    \n","    vectors = [] # positions in vector space\n","    labels = [] # keep track of words to label our data again later\n","    for word in model.wv.vocab:\n","        vectors.append(model.wv[word])\n","        labels.append(word)\n","        \n","        #convert both lists into numpy vectors for reduction\n","        vectors = np.array(vectors)\n","        #labels = np.asarray(labels)\n","        \n","        #reduce using t-SNE\n","        vectors = np.asarray(vectors)\n","        tsne =TSNE(n_components=num_dimentions,random_state=0)\n","        vectors = tsne.fit_transform(vectors)\n","        \n","        x_vals =[v[0] for v in vectors]\n","        y_vals =[v[1] for v in vectors]\n","        return x_vals, y_vals, labels\n","    \n","    x_vals, y_vals, labels = reduce_dimensions(reloaded_w2v_model)"]},{"cell_type":"markdown","metadata":{"id":"1vSUEyA__NRu"},"source":["Let us visualize our word2vec model code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRAoqRXF_NRu"},"outputs":[],"source":["def plot_with_matplotlib(x_vals, y_vals, labels):\n","    import matplotlib.pyplot as plt\n","    import random\n","    \n","    random.seed(0)\n","    \n","    plt.figure(figsize=(12,12))\n","    plt.scatter(x_vals,y_vals)\n","    \n","    #\n","    # Label randomly subsampled 25 data points\n","    \n","    indices = list(range(len(labels)))\n","    #selected_indices =  random.sample(indices, 25)\n","    selected_indices=[]\n","    index = labels.index(\"cell\")\n","    selected_indices.append(index)\n","    index = labels.index(\"phone\")\n","    selected_indices.append(index)\n","    index = labels.index(\"noise\")\n","    selected_indices.append(index)\n","    index = labels.index(\"cancellation\")\n","    selected_indices.append(index)\n","    index = labels.index(\"charger\")\n","    selected_indices.append(index)\n","    index = labels.index(\"charge\")\n","    selected_indices.append(index)\n","    index = labels.index(\"poor\")\n","    selected_indices.append(index)\n","    index = labels.index(\"bad\")\n","    selected_indices.append(index)\n","\n","    \n","    \n","    for i in selected_indices:\n","        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n","        \n","    \n","    \n","    plot_function = plot_with_matplotlib\n","    \n","    plot_function(x_vals, y_vals, labels)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}