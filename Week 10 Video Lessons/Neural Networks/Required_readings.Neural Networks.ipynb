{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57dfe0c-3d13-4a4f-ac85-0ea8551b1863",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef86e3-a181-4eef-a647-15ca752ce2aa",
   "metadata": {},
   "source": [
    "#### Perceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0ae3a-12ac-4306-bf3c-fb979a2fc28f",
   "metadata": {},
   "source": [
    "- The perceptron algorithm was introduced by Frank Rosenblatt in 1957.\n",
    "- The perceptron is a binary linear classifier that is only capable of predicting classes of samples if those samples\n",
    "can be separated via a straight line.\n",
    "- It classifies samples using hand crafted features which represents information about the samples, weighs the features \n",
    "on how important they are to the final prediction and the resulting computation is compared against a threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e2754-34b7-4546-ae3f-76b9c4062513",
   "metadata": {},
   "source": [
    "- A step function is an instant transformation of a value from 0 to 1. What this means is that if z is greater \n",
    "than or equal to 0, its predicts one class, else it predicts the other.\n",
    "- At each iteration, the predicted class gets compared to the actual class and the weights gets updated if the prediction \n",
    "was wrong else it is left unchanged in the case of a correct prediction. Updates of weights continue until all samples \n",
    "are correctly predicted, at which point we can say that the perceptron classifier has found a linear decision boundary\n",
    "that perfectly separates all samples into two mutually exclusive classes.\n",
    "During training the weights are updated by adding a small value to the original weights. The amount added is determined\n",
    "by the perceptron learning rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1cf42-be18-441c-bbd0-b03bcbdac99c",
   "metadata": {},
   "source": [
    "- The first coefficient on the right hand side of the equation is called the learning rate and acts as a scaling factor\n",
    "to increase or decrease the extent of the update.\n",
    "- It should be noted that the perceptron learning algorithm described is severely limited as it can only learn simple \n",
    "functions that have a clear linear boundary. The perceptron is almost never used in practice but served as an integral\n",
    "building block during the earlier development of artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5839b60-7230-410e-941f-99b8a675a844",
   "metadata": {},
   "source": [
    "#### Multi- Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c1a4cb-8dda-4256-8fa2-66b18adac5e3",
   "metadata": {},
   "source": [
    "- Modern iterations are known as multi-layer perceptrons. Multi-layer perceptrons are feed forward neural networks \n",
    "that have several nodes in the structure of a perceptron. However, there are important differences. \n",
    "- A multilayer perceptron  is made up of multiple layers of neurons stacked to form a network. The activation functions\n",
    "used are non-linear unlike the perceptron model that uses a step function. \n",
    "- Nonlinear activations are capable of capturing more interesting representations of data and as such do not require \n",
    "input data to be linearly separable. \n",
    "- The other important difference is that multi-layer perceptrons are trained using a different kind of algorithm called\n",
    "backpropagation which enables training across multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e756df-c633-4e34-bae1-24a46c309aec",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ca91e-ca8d-414a-9ec4-5a0b5ba35de4",
   "metadata": {},
   "source": [
    "##### Backpropagation is an algorithm technique that is used to solve the issue of credit assignment in artificial neural networks.\n",
    "##### What that means is that it is used to determine how much an input’s features and weights contribute to the final output of the model. Unlike the perceptron learning rule, Backpropagation is used to calculate the gradients, which tell us how much\n",
    "##### a change in the parameters of the model affects the final output. The gradients are used to train the model by using them as an error signal to indicate to the model how far off its predictions are from the ground truth.\n",
    "##### The backpropagation algorithm can be thought of as the chain rule of derivatives applied across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1629b5-dc6b-4184-939e-beb45932a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but '6642018e41afeb0956261959b29896376f1e3cd7-' is not\n",
      "  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f9df04-9cc0-4e1d-a9b3-68f84190587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762fe2e0-470d-4a70-bf1e-5c30d87dfe25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9188/1555846748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import MNIST data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tmp/data/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98db2b1a-1416-422f-9e71-07c986aa6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59b7646-2c43-44dd-bc93-42157454db26",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.datasets.mnist' has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mmnist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m8\u001b[39m], [\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m]), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.datasets.mnist' has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "plt.imshow(np.reshape(mnist.train.images[8], [28, 28]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8050f-c241-483d-88f4-94ca1bcb675a",
   "metadata": {},
   "source": [
    "In order to train an artificial neural network model on our data, we first need to define the parameters that describe\n",
    "the computation graph such as number of neurons in each hidden layer, number of hidden layers, input size, number of output \n",
    "classes etc. Each image in the dataset is 28 by 28 pixels therefore, the input shape is 784 which is 28 × 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161d45e-ae94-4a71-a8ec-08857f26ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed9f14-3268-4b9d-8fef-d2456c01bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 10 # 1st layer number of neurons\n",
    "n_hidden_2 = 10 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b5122-e5ba-422a-81db-c4530b3644d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975d129-b0ad-46d0-844c-15e0ad534542",
   "metadata": {},
   "source": [
    "We then declare weights and biases which are trainable parameters and initialise them randomly to very small values.\n",
    "The declarations are stored in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdd5e4-41b2-4fdf-bb12-4370fde7d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    " 'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    " 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    " 'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    " 'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    " 'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    " 'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9d948-c131-4f07-b929-4d3c52f036a8",
   "metadata": {},
   "source": [
    "We are would then describe a 3-layer neural network with 10 units in the\n",
    "output for each of the class digits and define the model by creating a\n",
    "function which forward propagates the inputs through the layers. Note that\n",
    "we are still describing all these operations on the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4988b4-d116-4699-899d-b92123f58711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    " # Hidden fully connected layer with 10 neurons\n",
    " layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    " # Hidden fully connected layer with 10 neurons\n",
    " layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    " # Output fully connected layer with a neuron for each class\n",
    " out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb64509-3133-498f-9f5b-1d1cb5bfc717",
   "metadata": {},
   "source": [
    "Next we call our function, define the loss objective, choose the optimizer that would be used to train the model \n",
    "and initialise all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3068f9-3c38-4ead-b095-c0f69232d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619862c-a56e-477c-b96f-847d2464512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    " logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf6e34-cf22-4267-9340-aeb23ac58a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a44a69-7f66-486c-9809-f2ee88d1b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a0164-9602-45b5-bed3-8561b7fce17b",
   "metadata": {},
   "source": [
    "Finally, we create a session, supply images in batches to the model for\n",
    "training and print the loss and accuracy for each mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246760e6-167e-4e4c-ad9e-7ae0a9bc8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077da9ed-1720-40c5-9cf0-dddd0d482376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the initializer\n",
    " sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0caef-7c79-4791-a86f-1c6bf56600c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    " # Run optimization op (backprop)\n",
    " sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    " # Calculate batch loss and accuracy\n",
    " loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                      Y: batch_y})\n",
    "    print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "          \"{:.3f}\".format(acc))\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59b686-a49a-495d-96f9-221436fcf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "          sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                        Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8c147-a8f6-4e7b-8593-0ac2003d470b",
   "metadata": {},
   "source": [
    "The loss drops to 0.4863 after training for 500 steps and we achieve an accuracy of 85% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cfc01-be3f-4b1f-b414-65971a2e11e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
