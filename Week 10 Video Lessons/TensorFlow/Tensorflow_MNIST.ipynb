{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a73d60-ecb7-404b-b79b-03af1a9ad722",
   "metadata": {},
   "source": [
    "## Week 10: Day 3 – Deeper Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504b052-099c-45c4-8001-9d413c9942a9",
   "metadata": {},
   "source": [
    "### Deep Neural Network For MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a05d4-3f77-465f-ba97-73d8ad372eb6",
   "metadata": {},
   "source": [
    "we will apply the knowledge from the lectures in this section to write a deep neural network.The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most  students if the first deep learning algorithm they use.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition.You can find more about  it on Yann LeCun's website(Director of AI Research,Facebook).\n",
    "\n",
    "The dataset provides 70,000 images (28×28 pixels) of handwritten digit (1 digit per image).\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0,1,2,3,4,5,6,7,8,9).this is a classification problem with 10 classes.\n",
    "\n",
    "Our goal would be build a neural network with 2 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5281c9-3e9d-4978-ad61-e7b5473c65a1",
   "metadata": {},
   "source": [
    "#### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eda56fad-4edb-468f-950a-f3a96825df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a72b35-8715-459b-bb55-86d5e593d253",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "#### Load a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afcc9db0-cb53-4a5d-9d8a-dfe0be414426",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tfds.loads(name) - loads a data from tensorflow dataset\n",
    "# tfds.loads(name,with_info,as_supervized) :- \n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated \n",
    "# (1) as supervised - True,loads the data in a 2-tuple structure(inputs,targets)\n",
    "# (2) with_info - True,provides a tuple containing information about version,features, #samples of the dataset\n",
    "mnist_dataset,mnist_info = tfds.load('mnist',with_info = True, as_supervised = True)\n",
    "\n",
    "mnist_train,mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "# this is also where we make use of mnist_info (we don't have to count the observations)\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['train'].num_examples\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "def scale(image, label):\n",
    "    # we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e05c64-cd50-4652-ba92-87e9bceedf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "# we have already decided that we will get the validation data from mnist_train\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# finally, we scale and batch the test data\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "test_dataset = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# then we can't shuffle the whole dataset in one go because we can't fit it all in memory\n",
    "# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n",
    "# if BUFFER_SIZE=1 => no shuffling will actually happen\n",
    "# if BUFFER_SIZE >= num samples => shuffling is uniform\n",
    "# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling\n",
    "\n",
    "# there is a shuffle method readily available and we just need to specify the buffer size\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "# finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db7adc64-4296-4b1c-9577-8c91669f5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) # Single batch, having size equal to number of validation samples\n",
    "\n",
    "# batch the test data\n",
    "test_dataset = test_dataset.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb1986-bf0d-470b-ab8d-70073cc628b5",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f0bdc-3e42-4d3d-97da-d1cd0cfd9bff",
   "metadata": {},
   "source": [
    "#### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e7ccd50-940f-4e5c-8c79-812eb20095f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # One for each pixel of the 28 * 28 image\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers.\n",
    "hidden_layer_size = 50 # Arbitrary chosen\n",
    "\n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # First hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfb32c-e79a-4d42-8347-1e7aced8c4fd",
   "metadata": {},
   "source": [
    "#### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09f89394-2832-40ba-a834-2a10cecf71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use, the loss function, and the metrics we are interested in obtaining at each iteration\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc535b2-501f-4b1f-b703-ba294db49311",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "828a139d-557c-47c2-9522-6a44573af836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 15s - loss: 0.4117 - accuracy: 0.8847 - val_loss: 0.2135 - val_accuracy: 0.9408\n",
      "Epoch 2/5\n",
      "540/540 - 1s - loss: 0.1886 - accuracy: 0.9449 - val_loss: 0.1815 - val_accuracy: 0.9462\n",
      "Epoch 3/5\n",
      "540/540 - 1s - loss: 0.1405 - accuracy: 0.9589 - val_loss: 0.1261 - val_accuracy: 0.9642\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.1137 - accuracy: 0.9656 - val_loss: 0.1149 - val_accuracy: 0.9672\n",
      "Epoch 5/5\n",
      "540/540 - 1s - loss: 0.0951 - accuracy: 0.9713 - val_loss: 0.0953 - val_accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22fe1ffe520>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "# we fit the model, specifying the training data the total number of epochs and the validation data we just created ourselves in the format:\n",
    "# (inputs,targets)\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94249afb-fb12-4ff4-bd28-6a363a1dc8f4",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "250227c2-f00a-4733-b28d-2b187903a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 39ms/step - loss: 0.1000 - accuracy: 0.9685\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c6f429c-58e9-4260-be84-d5e836c47faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10. Test accuracy: 96.85%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151c7cf-49e2-432a-aa47-add16ecefa68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
